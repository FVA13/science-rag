from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_openai import ChatOpenAI
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate
from langchain.schema import Document, SystemMessage
from typing import List, Dict, Any
import uuid
import fitz


class RAGSystem:
    def __init__(self, collection_name: str, embedding_model: str = "sentence-transformers/all-mpnet-base-v2", base_url: str = None, model_name=None):
        """
        Initialize the RAG system.

        :param collection_name: Name of the collection in ChromaDB.
        :param embedding_model: Model for text embedding (default is Sentence Transformers).
        :param base_url: Base URL for the LLM API (e.g., OpenAI).
        :param model_name: Name of the LLM model.
        """
        self.embeddings = HuggingFaceEmbeddings(model_name=embedding_model)
        self.vectorstore = Chroma(
            collection_name=collection_name,
            embedding_function=self.embeddings,
            persist_directory="./data/chroma_data"
        )
        self.llm = ChatOpenAI(base_url=base_url, model_name=model_name)

    def load_pdf(self, pdf_path: str, metadata: Dict[str, Any]):
        """
        Load a PDF document, split it into chunks, and index it in ChromaDB.

        :param pdf_path: Path to the PDF file.
        :param metadata: Metadata for the PDF (e.g., author, date, title).
        """
        pdf_document = fitz.open(pdf_path)

        all_text = ""
        page_texts = []
        for page_num, page in enumerate(pdf_document):
            text = page.get_text("text")
            page_texts.append(text)
            all_text += text + "\n"

        text_splitter = RecursiveCharacterTextSplitter(
            separators=["\n\n", "\n", " ", ""],
            chunk_size=1000,
            chunk_overlap=100,
        )
        chunks = text_splitter.split_text(all_text)

        documents = []
        for chunk in chunks:
            page_number = None
            for page_num, page_text in enumerate(page_texts, start=1):
                if chunk in page_text:
                    page_number = page_num

            # Add the chunk with metadata
            chunk_metadata = metadata.copy()
            chunk_metadata["page"] = page_number  # Add page number
            documents.append(Document(page_content=chunk, metadata=chunk_metadata))

        self.vectorstore.add_documents(documents)

    def search(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:
        """
        Search for relevant paragraphs based on the query.

        :param query: Query text.
        :param top_k: Number of most relevant paragraphs to return.
        :return: List of relevant paragraphs with metadata.
        """
        results = self.vectorstore.similarity_search_with_score(query, k=top_k)
        return [
            {
                "id": str(uuid.uuid4()),
                "document": result[0].page_content,
                "metadata": result[0].metadata,
                "score": result[1]
            }
            for result in results
        ]

    def correct_answer(self, initial_answer: str, context: str, query: str) -> str:
        """
        Correct the answer using Corrective RAG.

        :param initial_answer: Initial answer generated by the LLM.
        :param context: Context used to generate the answer.
        :param query: Original query.
        :return: Corrected answer.
        """
        correction_prompt_template = ChatPromptTemplate.from_messages([
            SystemMessage(content="You are a helper for correcting answers. Check if the answer matches the context and correct it if necessary."),
            HumanMessagePromptTemplate.from_template("Context:\n{context}\n\nQuestion:\n{query}\n\nInitial Answer:\n{initial_answer}\n\nCorrected Answer:")
        ])

        correction_prompt = correction_prompt_template.format_prompt(context=context, query=query, initial_answer=initial_answer)

        correction_response = self.llm.invoke(correction_prompt.to_messages())
        corrected_answer = correction_response.content

        return corrected_answer

    def generate_answer(self, query: str, top_k: int = 3) -> str:
        """
        Generate an answer to the question with references to sources.

        :param query: Query text.
        :param top_k: Number of most relevant paragraphs to use.
        :return: Answer with references to sources.
        """
        results = self.search(query, top_k)

        context = "\n".join([
            f"[{i+1}] {doc['document']} (Source: {doc['metadata'].get('source', 'Unknown')}, "
            f"Author: {doc['metadata'].get('author', 'Unknown')}, "
            f"Date: {doc['metadata'].get('date', 'Unknown')})"
            for i, doc in enumerate(results)
        ])
        print(f"CONTEXT: {context}")

        prompt_template = ChatPromptTemplate.from_messages([
            SystemMessage(content="You are a helper for answering scientific questions. Use the provided context to formulate the answer."),
            HumanMessagePromptTemplate.from_template("Context:\n{context}\n\nQuestion:\n{question}")
        ])

        prompt = prompt_template.format_prompt(context=context, question=query)

        response = self.llm.invoke(prompt.to_messages())

        initial_answer = response.content

        corrected_answer = self.correct_answer(initial_answer, context, query)

        sources = "\n\nSources:\n" + "\n".join([
            f"- {doc['metadata'].get('source', 'Unknown')} (Author: {doc['metadata'].get('author', 'Unknown')}, "
            f"Date: {doc['metadata'].get('date', 'Unknown')}, Page: {doc['metadata'].get('page', 'Unknown')})"
            for doc in results
        ])

        corrected_answer += sources

        return corrected_answer


if __name__ == "__main__":
    rag_system = RAGSystem(collection_name="science_articles", model_name="openai/gpt-4o-mini", base_url="https://api.vsegpt.ru/v1")

    pdf_path = "./data/raw/example_1.pdf"
    metadata = {
        "source": "example.pdf",
        "author": "John Doe",
        "date": "2023-10-01"
    }
    rag_system.load_pdf(pdf_path, metadata)
    answer = rag_system.generate_answer(query="What algorithms does the federated learning paradigm approach compare to?")
    print(answer)